{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9467cb1a-523b-43a7-b1e6-f14bdaa9bc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e39afc-e504-4120-894e-2c486af7b5af",
   "metadata": {},
   "source": [
    "#### Specify location of COCO data. If location corresponds to training or validation data then there should be an annotations file<br>for creating training targets. For testing data there is no annotation file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb7f9f5-7549-4d49-9acc-0f8e69fd9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/Data/COCO dataset/'\n",
    "DATA_TYPE = 'val2017'\n",
    "ANN_FILE =f'{DATA_DIR}/annotations/instances_{DATA_TYPE}.json'\n",
    "OUTPUT_FILE = f'{DATA_DIR}results/result_{DATA_TYPE}.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f8119-c2ae-45fd-b1b8-76c5eb2a62b8",
   "metadata": {},
   "source": [
    "#### Create Dataset class for making data from images. Images don't have to be all same size so we must return lists of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86624cc-58ea-4123-9c99-9cc7dbfdea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataDir=DATA_DIR,dataType=DATA_TYPE):\n",
    "        try:\n",
    "            self.annFile = f'{dataDir}/annotations/instances_{dataType}.json'\n",
    "            self.coco = COCO(self.annFile)\n",
    "            self.catIds = self.coco.getCatIds()\n",
    "            self.have_ann = True\n",
    "        except:\n",
    "            print('No annotations file found')\n",
    "            self.have_ann = False\n",
    "        self.img_names = os.listdir(f'{dataDir}{dataType}')\n",
    "        self.pref = os.path.join(dataDir,dataType)\n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.img_names[idx].partition('.')[0]\n",
    "        img_id = int(img_name)\n",
    "        if self.have_ann:\n",
    "            img_info = self.coco.imgs[img_id]\n",
    "            fname = img_info['file_name']\n",
    "        else:\n",
    "            fname = str(img_id)\n",
    "            fname = '0'*(12-len(fname))+fname+'.jpg'\n",
    "        # load in image data\n",
    "        img_mat = io.imread(os.path.join(self.pref,fname))\n",
    "        if img_mat.ndim != 3:\n",
    "            return None,None\n",
    "        img_torch = torch.tensor(img_mat.transpose((2,0,1))).cuda()/255\n",
    "        # load in annotations if annotation file exists\n",
    "        if self.have_ann:\n",
    "            annIds = self.coco.getAnnIds(imgIds=img_id, catIds=self.catIds, iscrowd=None)\n",
    "            anns = self.coco.loadAnns(annIds)\n",
    "            if len(anns) == 0:\n",
    "                return None,None\n",
    "            masks = [self.coco.annToMask(ann) for ann in anns]\n",
    "            targets = {k:[] for k in anns[0].keys()}\n",
    "            targets.pop('segmentation')\n",
    "            for ann in anns:\n",
    "                for k,v in ann.items():\n",
    "                    if k != 'segmentation':\n",
    "                        targets[k].append(v)\n",
    "            targets2 = {k:torch.tensor(v).cuda() for k,v in targets.items()}\n",
    "            targets2['bbox'][:,2:] += targets2['bbox'][:,:2]\n",
    "            targets2['boxes'] = targets2.pop('bbox')\n",
    "            targets2['labels'] = targets2.pop('category_id')\n",
    "        else:\n",
    "            ## if annotations are not available make dummy target that keeps image identification informatoin\n",
    "            targets2 = {'img_id':torch.tensor([img_id])}\n",
    "        return img_torch,targets2\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb383bf2-db84-450c-88d3-312dc305bb77",
   "metadata": {},
   "source": [
    "#### Create dataloader for COCO data. Since data is in lists we must make a custom collate_fn to prevent default collate_fn <br> from trying to stack lists into a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d42e31-e914-4d7f-b9c1-d687be9a2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = [v[0] for v in batch if v[0] is not None]\n",
    "    targets = [v[1] for v in batch if v[0] is not None]\n",
    "    return data,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5625127c-dae0-4803-9e6e-07def02c2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_data = COCO_Dataset(DATA_DIR,DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bf1e26-e738-4b64-b094-f676653b38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_loader = torch.utils.data.DataLoader(coco_data,batch_size=8,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40039b04-0c9f-42eb-a11c-970ad22a259f",
   "metadata": {},
   "source": [
    "#### Function that takes model output and converts it to format specified by COCO to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448fd0be-5cf9-4efd-822f-83edaf72856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools\n",
    "def process_output(outs,targets,results):\n",
    "    for out,target in zip(outs,targets):\n",
    "        image_id = target['image_id'][0].item()\n",
    "        boxes = out['boxes'].detach().cpu().numpy()\n",
    "        boxes[:,2:] -= boxes[:,:2]\n",
    "        if 'masks' in out:\n",
    "            masks = np.asfortranarray(out['masks'].detach().cpu().numpy().transpose((1,2,0)).astype(bool).astype(np.uint8))\n",
    "            have_masks = True\n",
    "        else:\n",
    "            have_masks = False                                \n",
    "        for ind in range(len(boxes)):\n",
    "            results.append({'image_id':image_id,'score':out['scores'][ind].item(),\n",
    "                            'category_id':out['labels'][ind].item(),'bbox':boxes[ind].tolist(),\n",
    "                            'segmentation':None if not have_masks else masks[ind]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea197cc-107f-48ef-acf5-3665ac169bc8",
   "metadata": {},
   "source": [
    "#### Load in the appropriate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2de2db7d-5802-45c5-b5e1-28122922a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights, FasterRCNN_ResNet50_FPN_Weights\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef789e-04b8-4be4-93a8-60170bc36de3",
   "metadata": {},
   "source": [
    "#### Evaluation step. Apply model to every data sample, convert to COCO output format and save in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ad1246-4bc2-4ee8-8f44-16da22810e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225.59921836853027\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "results = []\n",
    "model.eval();\n",
    "for imgs,targets in coco_loader:\n",
    "    model_outputs = model(imgs,targets)\n",
    "    torch.cuda.empty_cache()\n",
    "    process_output(model_outputs,targets,results)\n",
    "print(time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5430bc70-6bde-4ee3-8a94-0cf62e26651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(OUTPUT_FILE,'w') as ff:\n",
    "    json.dump(results,ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7033935-cade-4d2b-b7a5-d85df7c1a412",
   "metadata": {},
   "source": [
    "#### Use pycocotools evaluation functionality to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d26277-fa90-4e20-97e0-6ae3c8418711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.74s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=17.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=2.87s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.369\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.397\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.212\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.403\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.307\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.508\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.317\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.544\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.645\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "annType = 'bbox' # annType should be 'bbox' for bounding box and 'segm' for segmentation\n",
    "cocoGt = COCO(ANN_FILE)\n",
    "cocoDt = cocoGt.loadRes(OUTPUT_FILE)\n",
    "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
    "cocoEval.params.imgIds  = cocoGt.getImgIds()\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
